# ðŸ“Š ëª¨ë‹ˆí„°ë§ ì „ëžµ

> Korea Public Data í”„ë¡œì íŠ¸ì˜ ì¢…í•©ì ì¸ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë° ê´€ì¸¡ ê°€ëŠ¥ì„± ì „ëžµ

## ðŸ“‹ ëª©ì°¨
- [ëª¨ë‹ˆí„°ë§ ê°œìš”](#ëª¨ë‹ˆí„°ë§-ê°œìš”)
- [ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ](#ëª¨ë‹ˆí„°ë§-ìŠ¤íƒ)
- [ë©”íŠ¸ë¦­ ìˆ˜ì§‘](#ë©”íŠ¸ë¦­-ìˆ˜ì§‘)
- [ë¡œê¹… ì „ëžµ](#ë¡œê¹…-ì „ëžµ)
- [ì•Œë¦¼ ë° ê²½ê³ ](#ì•Œë¦¼-ë°-ê²½ê³ )
- [ëŒ€ì‹œë³´ë“œ êµ¬ì„±](#ëŒ€ì‹œë³´ë“œ-êµ¬ì„±)
- [SLA/SLO ì •ì˜](#sla-slo-ì •ì˜)
- [ì„±ëŠ¥ ì¶”ì ](#ì„±ëŠ¥-ì¶”ì )

## ðŸŽ¯ ëª¨ë‹ˆí„°ë§ ê°œìš”

### ëª¨ë‹ˆí„°ë§ ëª©í‘œ
1. **Proactive Issue Detection**: ë¬¸ì œ ì‚¬ì „ ê°ì§€ ë° ì˜ˆë°©
2. **Performance Optimization**: ì„±ëŠ¥ ë³‘ëª© ì§€ì  ì‹ë³„ ë° ê°œì„ 
3. **User Experience**: ì‚¬ìš©ìž ê²½í—˜ í’ˆì§ˆ ì¸¡ì •
4. **Business Metrics**: ë¹„ì¦ˆë‹ˆìŠ¤ KPI ì¶”ì 

### Three Pillars of Observability
```mermaid
graph TD
    A[Observability] --> B[Metrics]
    A --> C[Logs]
    A --> D[Traces]
    
    B --> E[System Health]
    B --> F[Performance]
    B --> G[Business KPIs]
    
    C --> H[Error Tracking]
    C --> I[Audit Trail]
    C --> J[Debug Info]
    
    D --> K[Request Flow]
    D --> L[Latency Analysis]
    D --> M[Service Dependencies]
```

## ðŸ›  ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ

### í•µì‹¬ ë„êµ¬
```yaml
monitoring_stack:
  metrics:
    - name: "Prometheus"
      purpose: "ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì €ìž¥"
      retention: "30ì¼"
    
    - name: "Grafana"
      purpose: "ì‹œê°í™” ë° ëŒ€ì‹œë³´ë“œ"
      version: "10.x"
  
  logs:
    - name: "Fluent Bit"
      purpose: "ë¡œê·¸ ìˆ˜ì§‘"
      config: "kubernetes-optimized"
    
    - name: "Elasticsearch"
      purpose: "ë¡œê·¸ ì €ìž¥ ë° ê²€ìƒ‰"
      retention: "90ì¼"
    
    - name: "Kibana"
      purpose: "ë¡œê·¸ ë¶„ì„ ë° ì‹œê°í™”"
  
  traces:
    - name: "Jaeger"
      purpose: "ë¶„ì‚° íŠ¸ë ˆì´ì‹±"
      sampling_rate: "1%"
  
  alerting:
    - name: "Alertmanager"
      purpose: "ì•Œë¦¼ ê´€ë¦¬"
      channels: ["Slack", "Email", "PagerDuty"]
```

### ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ëž¨
```mermaid
graph TB
    subgraph "Application Layer"
        A[Frontend] --> B[Backend API]
        B --> C[Database]
        B --> D[Redis]
    end
    
    subgraph "Monitoring Layer"
        A --> E[Prometheus]
        B --> E
        C --> E
        D --> E
        
        A --> F[Fluent Bit]
        B --> F
        
        E --> G[Grafana]
        F --> H[Elasticsearch]
        H --> I[Kibana]
        
        E --> J[Alertmanager]
        J --> K[Slack/Email]
    end
```

## ðŸ“ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

### Application Metrics
```typescript
// Frontend - Next.js ë©”íŠ¸ë¦­
// fe/src/lib/monitoring/metrics.ts
import { NextWebVitalsMetric } from 'next/app';

export function reportWebVitals(metric: NextWebVitalsMetric) {
  const url = '/api/analytics';
  
  const body = JSON.stringify({
    metric_name: metric.name,
    metric_value: metric.value,
    metric_id: metric.id,
    page: window.location.pathname,
    timestamp: Date.now()
  });

  // Use sendBeacon for reliability
  if (navigator.sendBeacon) {
    navigator.sendBeacon(url, body);
  } else {
    fetch(url, { body, method: 'POST' });
  }
}

// Custom business metrics
export const trackUserAction = (action: string, properties?: object) => {
  // Track user interactions for business intelligence
  window.gtag?.('event', action, {
    ...properties,
    timestamp: Date.now()
  });
};
```

```python
# Backend - FastAPI ë©”íŠ¸ë¦­
# be/app/core/metrics.py
from prometheus_client import Counter, Histogram, Gauge
import time
from functools import wraps

# Business Metrics
api_requests_total = Counter(
    'api_requests_total',
    'Total API requests',
    ['method', 'endpoint', 'status_code']
)

api_request_duration = Histogram(
    'api_request_duration_seconds',
    'API request duration',
    ['method', 'endpoint']
)

active_users = Gauge(
    'active_users_total',
    'Number of active users',
    ['time_window']
)

# Database Metrics
db_operations_total = Counter(
    'db_operations_total',
    'Total database operations',
    ['operation', 'collection', 'status']
)

db_connection_pool = Gauge(
    'db_connection_pool_size',
    'Database connection pool size',
    ['status']
)

def track_api_metrics(func):
    """Decorator to track API endpoint metrics"""
    @wraps(func)
    async def wrapper(request, *args, **kwargs):
        start_time = time.time()
        status_code = 200
        
        try:
            response = await func(request, *args, **kwargs)
            status_code = response.status_code
            return response
        except Exception as e:
            status_code = 500
            raise
        finally:
            duration = time.time() - start_time
            
            api_requests_total.labels(
                method=request.method,
                endpoint=request.url.path,
                status_code=status_code
            ).inc()
            
            api_request_duration.labels(
                method=request.method,
                endpoint=request.url.path
            ).observe(duration)
    
    return wrapper
```

### Infrastructure Metrics
```yaml
# k8s/monitoring/prometheus-config.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  # Kubernetes API server
  - job_name: 'kubernetes-apiservers'
    kubernetes_sd_configs:
      - role: endpoints
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    
  # Node Exporter
  - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
      - role: node
    relabel_configs:
      - source_labels: [__address__]
        regex: '(.*):10250'
        target_label: __address__
        replacement: '${1}:9100'
    
  # Application Pods
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
```

## ðŸ“ ë¡œê¹… ì „ëžµ

### êµ¬ì¡°í™”ëœ ë¡œê¹…
```python
# be/app/core/logging_config.py
import logging
import json
import sys
from datetime import datetime
from typing import Dict, Any

class StructuredLogger:
    def __init__(self, name: str, level: str = "INFO"):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(getattr(logging, level.upper()))
        
        # JSON formatter for structured logging
        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(JsonFormatter())
        self.logger.addHandler(handler)
    
    def log(self, level: str, message: str, **kwargs):
        extra = {
            'timestamp': datetime.utcnow().isoformat(),
            'service': 'korea-backend',
            'environment': settings.environment,
            **kwargs
        }
        getattr(self.logger, level.lower())(message, extra=extra)

class JsonFormatter(logging.Formatter):
    def format(self, record):
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': record.levelname,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno,
        }
        
        # Add extra fields
        if hasattr(record, 'extra'):
            log_entry.update(record.extra)
        
        return json.dumps(log_entry)

# Usage example
logger = StructuredLogger('api')

@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    process_time = time.time() - start_time
    
    logger.log('info', 'API request processed', 
        method=request.method,
        url=str(request.url),
        status_code=response.status_code,
        process_time=process_time,
        user_agent=request.headers.get('user-agent'),
        client_ip=request.client.host
    )
    
    return response
```

### ë¡œê·¸ ìˆ˜ì§‘ ì„¤ì •
```yaml
# logging/fluent-bit-config.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: logging
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         5
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf

    [INPUT]
        Name              tail
        Path              /var/log/containers/*korea*.log
        Parser            docker
        Tag               korea.*
        Refresh_Interval  5
        Mem_Buf_Limit     50MB

    [FILTER]
        Name                kubernetes
        Match               korea.*
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Merge_Log           On
        K8S-Logging.Parser  On

    [FILTER]
        Name                parser
        Match               korea.*
        Key_Name            log
        Parser              json

    [OUTPUT]
        Name            es
        Match           *
        Host            elasticsearch.logging.svc.cluster.local
        Port            9200
        Index           korea-logs-${ENVIRONMENT}
        Type            _doc
        Logstash_Format On
        Retry_Limit     False
```

## ðŸš¨ ì•Œë¦¼ ë° ê²½ê³ 

### ì•Œë¦¼ ê·œì¹™ ì •ì˜
```yaml
# monitoring/alert-rules.yml
groups:
  - name: korea-public-data.rules
    rules:
      # Infrastructure Alerts
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 80% for more than 5 minutes"

      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is above 85% for more than 5 minutes"

      # Application Alerts
      - alert: HighErrorRate
        expr: rate(api_requests_total{status_code=~"5.."}[5m]) / rate(api_requests_total[5m]) > 0.05
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 5% for more than 2 minutes"

      - alert: HighResponseTime
        expr: histogram_quantile(0.95, api_request_duration_seconds) > 2
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is above 2 seconds"

      # Business Alerts
      - alert: LowUserActivity
        expr: active_users_total{time_window="1h"} < 10
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Low user activity detected"
          description: "Less than 10 active users in the last hour"

      - alert: DatabaseConnectionIssue
        expr: db_connection_pool_size{status="available"} / db_connection_pool_size{status="total"} < 0.2
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Database connection pool exhausted"
          description: "Less than 20% of database connections available"
```

### ì•Œë¦¼ ì±„ë„ ì„¤ì •
```yaml
# monitoring/alertmanager.yml
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@korea-public-data.com'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default-receiver'
  routes:
    - match:
        severity: critical
      receiver: 'critical-alerts'
    - match:
        severity: warning
      receiver: 'warning-alerts'

receivers:
  - name: 'default-receiver'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'critical-alerts'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_CRITICAL}'
        channel: '#critical-alerts'
        title: 'ðŸš¨ CRITICAL: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    email_configs:
      - to: 'oncall@korea-public-data.com'
        subject: 'CRITICAL Alert: {{ .GroupLabels.alertname }}'
        body: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'warning-alerts'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#monitoring'
        title: 'âš ï¸ WARNING: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
```

## ðŸ“Š ëŒ€ì‹œë³´ë“œ êµ¬ì„±

### System Overview Dashboard
```json
{
  "dashboard": {
    "title": "Korea Public Data - System Overview",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(api_requests_total[5m])) by (endpoint)",
            "legendFormat": "{{endpoint}}"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, api_request_duration_seconds)",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, api_request_duration_seconds)",
            "legendFormat": "50th percentile"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "singlestat",
        "targets": [
          {
            "expr": "rate(api_requests_total{status_code=~\"5..\"}[5m]) / rate(api_requests_total[5m]) * 100"
          }
        ]
      }
    ]
  }
}
```

### Business Metrics Dashboard
```json
{
  "dashboard": {
    "title": "Korea Public Data - Business Metrics",
    "panels": [
      {
        "title": "Active Users",
        "type": "graph",
        "targets": [
          {
            "expr": "active_users_total",
            "legendFormat": "{{time_window}}"
          }
        ]
      },
      {
        "title": "Popular Announcements",
        "type": "table",
        "targets": [
          {
            "expr": "topk(10, announcement_views_total)"
          }
        ]
      },
      {
        "title": "Search Queries",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(search_queries_total[5m])"
          }
        ]
      }
    ]
  }
}
```

## ðŸ“ SLA/SLO ì •ì˜

### Service Level Objectives
```yaml
slo_definitions:
  availability:
    target: "99.9%"
    measurement_window: "30d"
    error_budget: "0.1%"
    
  latency:
    target: "95% of requests < 500ms"
    measurement_window: "7d"
    
  throughput:
    target: "> 1000 requests/min peak"
    measurement_window: "24h"
    
  error_rate:
    target: "< 1% error rate"
    measurement_window: "24h"

sli_queries:
  availability:
    good_events: "sum(rate(api_requests_total{status_code!~'5..'}[30d]))"
    total_events: "sum(rate(api_requests_total[30d]))"
    
  latency:
    good_events: "sum(rate(api_request_duration_seconds_bucket{le='0.5'}[7d]))"
    total_events: "sum(rate(api_request_duration_seconds_count[7d]))"
    
  error_rate:
    bad_events: "sum(rate(api_requests_total{status_code=~'5..'}[24h]))"
    total_events: "sum(rate(api_requests_total[24h]))"
```

## ðŸŽ¯ ì„±ëŠ¥ ì¶”ì 

### Frontend ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```typescript
// fe/src/lib/performance/web-vitals.ts
import { getCLS, getFID, getFCP, getLCP, getTTFB } from 'web-vitals';

export function initializeWebVitals() {
  getCLS(sendToAnalytics);
  getFID(sendToAnalytics);
  getFCP(sendToAnalytics);
  getLCP(sendToAnalytics);
  getTTFB(sendToAnalytics);
}

function sendToAnalytics(metric: any) {
  const body = JSON.stringify({
    name: metric.name,
    value: metric.value,
    id: metric.id,
    url: window.location.href,
    timestamp: Date.now()
  });

  navigator.sendBeacon('/api/web-vitals', body);
}

// Real User Monitoring
export function trackPageLoad() {
  const navigationEntry = performance.getEntriesByType('navigation')[0] as PerformanceNavigationTiming;
  
  const metrics = {
    dns_time: navigationEntry.domainLookupEnd - navigationEntry.domainLookupStart,
    connect_time: navigationEntry.connectEnd - navigationEntry.connectStart,
    request_time: navigationEntry.responseEnd - navigationEntry.requestStart,
    dom_content_loaded: navigationEntry.domContentLoadedEventEnd - navigationEntry.domContentLoadedEventStart,
    load_complete: navigationEntry.loadEventEnd - navigationEntry.loadEventStart
  };

  fetch('/api/performance', {
    method: 'POST',
    body: JSON.stringify(metrics),
    headers: { 'Content-Type': 'application/json' }
  });
}
```

### Backend ì„±ëŠ¥ ì¶”ì 
```python
# be/app/core/performance.py
import asyncio
import time
from contextlib import asynccontextmanager
from typing import Dict, Any

class PerformanceMonitor:
    def __init__(self):
        self.active_requests = {}
        self.slow_queries = []
    
    @asynccontextmanager
    async def track_request(self, request_id: str, metadata: Dict[str, Any]):
        start_time = time.time()
        self.active_requests[request_id] = {
            'start_time': start_time,
            'metadata': metadata
        }
        
        try:
            yield
        finally:
            duration = time.time() - start_time
            self.active_requests.pop(request_id, None)
            
            # Track slow requests
            if duration > 1.0:  # > 1 second
                self.slow_queries.append({
                    'request_id': request_id,
                    'duration': duration,
                    'metadata': metadata,
                    'timestamp': start_time
                })
    
    async def get_performance_stats(self) -> Dict[str, Any]:
        return {
            'active_requests': len(self.active_requests),
            'slow_queries_last_hour': len([
                q for q in self.slow_queries 
                if time.time() - q['timestamp'] < 3600
            ]),
            'average_response_time': await self._get_avg_response_time()
        }

performance_monitor = PerformanceMonitor()
```

## ðŸ“‹ ëª¨ë‹ˆí„°ë§ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì¼ì¼ ëª¨ë‹ˆí„°ë§ ìž‘ì—…
- [ ] ì‹œìŠ¤í…œ ëŒ€ì‹œë³´ë“œ í™•ì¸
- [ ] ì•Œë¦¼ í˜„í™© ê²€í† 
- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„
- [ ] ë¡œê·¸ ì—ëŸ¬ íŒ¨í„´ í™•ì¸
- [ ] ì‚¬ìš©ìž í™œë™ ì§€í‘œ ê²€í† 

### ì£¼ê°„ ëª¨ë‹ˆí„°ë§ ìž‘ì—…
- [ ] SLO ë‹¬ì„± í˜„í™© í‰ê°€
- [ ] ìš©ëŸ‰ ê³„íš ê²€í† 
- [ ] ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
- [ ] ì•Œë¦¼ ê·œì¹™ ìµœì í™”
- [ ] ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸

### ì›”ê°„ ëª¨ë‹ˆí„°ë§ ìž‘ì—…
- [ ] ëª¨ë‹ˆí„°ë§ ì „ëžµ ê²€í† 
- [ ] ë„êµ¬ ì—…ë°ì´íŠ¸ ê³„íš
- [ ] ì„±ëŠ¥ ê¸°ì¤€ì„  ìž¬ì„¤ì •
- [ ] ìš´ì˜ í”„ë¡œì„¸ìŠ¤ ê°œì„ 
- [ ] ëª¨ë‹ˆí„°ë§ êµìœ¡ ì‹¤ì‹œ

## ðŸ”„ ì—…ë°ì´íŠ¸ ì´ë ¥

| ë²„ì „ | ë‚ ì§œ | ë³€ê²½ì‚¬í•­ | ìž‘ì„±ìž |
|------|------|----------|--------|
| 1.0.0 | 2025-08-14 | ì´ˆê¸° ëª¨ë‹ˆí„°ë§ ì „ëžµ ìˆ˜ë¦½ | PM |

---

*ë³¸ ëª¨ë‹ˆí„°ë§ ì „ëžµì€ ì‹œìŠ¤í…œ í™•ìž¥ê³¼ ìš”êµ¬ì‚¬í•­ ë³€í™”ì— ë”°ë¼ ì§€ì†ì ìœ¼ë¡œ ê°œì„ ë˜ë©°, ëª¨ë“  íŒ€ì›ì€ ëŠ¥ë™ì ì¸ ëª¨ë‹ˆí„°ë§ ë¬¸í™”ë¥¼ ì‹¤ì²œí•´ì•¼ í•©ë‹ˆë‹¤.*